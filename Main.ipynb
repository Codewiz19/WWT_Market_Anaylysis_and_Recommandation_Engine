{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codewiz19/WWT_Team_CTRL-Z-Mandal-/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap1BvE_-JcuG"
      },
      "source": [
        "# End-to-End Solution for WWT Competition 2025\n",
        "\n",
        "This notebook provides a complete, end-to-end implementation of a sophisticated hybrid recommendation engine designed to increase average order value for Wings R Us. It follows the strategic blueprint outlined in the project documentation, combining multiple powerful recommendation techniques to deliver personalized, relevant, and novel suggestions.\n",
        "\n",
        "System Architecture:\n",
        "The core of this solution is a hybrid model that orchestrates five distinct algorithms, all trained on rich, context-aware data:\n",
        "\n",
        "    ğŸ›’ Market Basket Analysis (Apriori): Uncovers foundational 'if-then' purchasing patterns between specific items (e.g., customers who buy \"10 pc Spicy Wings\" also buy \"Ranch Dip - Regular\").\n",
        "\n",
        "    ğŸ“ Content-Based Filtering (TF-IDF): Recommends items by matching a detailed profile of the current order, including the specific items, customer type, order occasion (ToGo/Delivery), and city.\n",
        "\n",
        "    ğŸ§  Factorization Machines (FM): A powerful deep learning model that learns personalized tastes by modeling the interactions between a user's entire purchase history and specific menu items.\n",
        "\n",
        "    ğŸ¤– Neural Collaborative Filtering (NCF): The most advanced component, using deep learning to capture subtle, non-linear user preferences for true 1-to-1 personalization.\n",
        "\n",
        "    ğŸ“ˆ Popularity-Based Fallback: A robust safety net that ensures all users receive sensible, crowd-pleasing recommendations when other models cannot find a strong signal.\n",
        "\n",
        "**Key Innovations Implemented:**\n",
        "\n",
        "    Context-Rich Feature Engineering: We've moved beyond simple item names. The model now learns from a detailed profile for each transaction that includes the customer type, order occasion (ToGo/Delivery), and the specific city of the store.\n",
        "\n",
        "    Specific Item Recommendations: The models are trained directly on orderable item names (e.g., \"10 pc Spicy Wings Combo\"), allowing them to make specific, actionable recommendations without a secondary mapping step.\n",
        "\n",
        "    Location-Aware Suggestions: By incorporating the city into the item profiles, the system can learn and recommend items based on local and regional taste preferences.\n",
        "\n",
        "    Full Cold-Start Handling: The hybrid design ensures effective recommendations for new customers and newly added menu items by gracefully falling back from personalized models to cart-based and popularity-based logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrx3m3vsJcuM"
      },
      "source": [
        "## Part 1: Setup and Data Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrcpLSZ5JcuO"
      },
      "source": [
        "### 1.1: Install and Import Libraries\n",
        "First, we install and import all the necessary Python libraries. This includes standard data manipulation tools, machine learning libraries, and specific packages for association rule mining and deep learning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn mlxtend torch -q"
      ],
      "metadata": {
        "id": "peyQvO_6JgnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c88a93b0-b600-4360-ad63-25a95df1619b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H8ejkS6RJcuO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "# Scikit-learn for content-based filtering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# MLxtend for Market Basket Analysis\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# PyTorch for Deep Learning Models (FM and NCF)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FewTfbpZJzC4",
        "outputId": "6ac9861d-238f-44bd-c3be-47aecf7e6109"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-1OXr1NJcuR"
      },
      "source": [
        "### 1.2: Load Datasets\n",
        "We now load all the provided data files into pandas DataFrames. We'll also perform an initial inspection (`.info()`, `.head()`) to understand their structure and identify any immediate data quality issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT7Wd1KLJcuS",
        "outputId": "12b5d338-307a-48e6-b5df-5d6ffed02fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "--- Order Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1048575 entries, 0 to 1048574\n",
            "Data columns (total 8 columns):\n",
            " #   Column                 Non-Null Count    Dtype \n",
            "---  ------                 --------------    ----- \n",
            " 0   CUSTOMER_ID            1048575 non-null  int64 \n",
            " 1   STORE_NUMBER           1048575 non-null  int64 \n",
            " 2   ORDER_CREATED_DATE     1048575 non-null  object\n",
            " 3   ORDER_ID               1048575 non-null  int64 \n",
            " 4   ORDERS                 1048575 non-null  object\n",
            " 5   ORDER_CHANNEL_NAME     1048575 non-null  object\n",
            " 6   ORDER_SUBCHANNEL_NAME  1048575 non-null  object\n",
            " 7   ORDER_OCCASION_NAME    1048575 non-null  object\n",
            "dtypes: int64(3), object(5)\n",
            "memory usage: 64.0+ MB\n",
            "\n",
            "--- Customer Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 563346 entries, 0 to 563345\n",
            "Data columns (total 2 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   CUSTOMER_ID    563346 non-null  int64 \n",
            " 1   CUSTOMER_TYPE  563319 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 8.6+ MB\n",
            "\n",
            "--- Store Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 38 entries, 0 to 37\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   STORE_NUMBER  38 non-null     int64 \n",
            " 1   CITY          25 non-null     object\n",
            " 2   STATE         24 non-null     object\n",
            " 3   POSTAL_CODE   36 non-null     object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 1.3+ KB\n",
            "\n",
            "--- Test Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 10 columns):\n",
            " #   Column                 Non-Null Count  Dtype \n",
            "---  ------                 --------------  ----- \n",
            " 0   CUSTOMER_ID            1000 non-null   int64 \n",
            " 1   STORE_NUMBER           1000 non-null   int64 \n",
            " 2   ORDER_ID               1000 non-null   int64 \n",
            " 3   ORDER_CHANNEL_NAME     1000 non-null   object\n",
            " 4   ORDER_SUBCHANNEL_NAME  1000 non-null   object\n",
            " 5   ORDER_OCCASION_NAME    1000 non-null   object\n",
            " 6   CUSTOMER_TYPE          1000 non-null   object\n",
            " 7   item1                  1000 non-null   object\n",
            " 8   item2                  1000 non-null   object\n",
            " 9   item3                  1000 non-null   object\n",
            "dtypes: int64(3), object(7)\n",
            "memory usage: 78.3+ KB\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/'\n",
        "\n",
        "order_data = pd.read_csv(file_path + 'order_data.csv')\n",
        "customer_data = pd.read_csv(file_path + 'customer_data.csv')\n",
        "store_data = pd.read_csv(file_path + 'store_data.csv')\n",
        "test_data = pd.read_csv(file_path + 'test_data_question.csv')\n",
        "\n",
        "\n",
        "print(\"\\n--- Order Data Info ---\")\n",
        "order_data.info()\n",
        "print(\"\\n--- Customer Data Info ---\")\n",
        "customer_data.info()\n",
        "print(\"\\n--- Store Data Info ---\")\n",
        "store_data.info()\n",
        "print(\"\\n--- Test Data Info ---\")\n",
        "test_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVziqx9dJcuT"
      },
      "source": [
        "## Data Preprocessing and Feature Engineering\n",
        "\n",
        "This is the most critical stage. The quality of our recommendations depends entirely on the quality of our data. The following steps are not just a simple cleaning process; they are a direct result of insights gained during our initial Exploratory Data Analysis (EDA).\n",
        "\n",
        "Our EDA revealed several key challenges and opportunities:\n",
        "\n",
        "    1. The raw order data is nested within complex JSON strings, requiring careful parsing.\n",
        "    2. The data contains significant noise, including non-food items like 'memos' and 'tips' that must be filtered out.\n",
        "    3. Rich contextual information, such as the store's city, the customer's type, and the order occasion (ToGo/Delivery), is available and crucial for building a nuanced model.\n",
        "\n",
        "Based on these findings, we will implement an advanced preprocessing strategy to create a clean, structured, and intelligent dataset ready for our hybrid recommendation engine."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assume order_data, customer_data, and store_data are already loaded\n",
        "\n",
        "def clean_item_name(name):\n",
        "    \"\"\"Standardizes item names by converting to lowercase and removing special characters.\"\"\"\n",
        "    if not isinstance(name, str):\n",
        "        return \"\"\n",
        "    name = name.lower()\n",
        "    name = re.sub(r'[^a-z0-9\\s]', '', name)\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    return name\n",
        "\n",
        "def is_system_item(item):\n",
        "    \"\"\"Identifies and filters out non-product items.\"\"\"\n",
        "    system_keywords = [\n",
        "        'tip', 'fee', 'bag', 'delivery', 'service', 'charge',\n",
        "        'memo', 'blankline', 'asap', 'paid', 'subtotal', 'tax'\n",
        "    ]\n",
        "    name_to_check = \"\"\n",
        "    if isinstance(item, dict):\n",
        "        name_to_check = item.get('item_name', '').lower()\n",
        "    elif isinstance(item, str):\n",
        "        name_to_check = item.lower()\n",
        "    if not name_to_check:\n",
        "        return False\n",
        "    return any(keyword in name_to_check for keyword in system_keywords)\n",
        "\n",
        "def parse_order_json(orders_json):\n",
        "    \"\"\"Parses the nested JSON string to extract a list of clean item names.\"\"\"\n",
        "    if not isinstance(orders_json, str):\n",
        "        return []\n",
        "    try:\n",
        "        data = json.loads(orders_json)\n",
        "        if 'orders' in data and isinstance(data['orders'], list) and data['orders']:\n",
        "            if 'item_details' in data['orders'][0] and isinstance(data['orders'][0]['item_details'], list):\n",
        "                item_list = data['orders'][0]['item_details']\n",
        "            else:\n",
        "                return []\n",
        "        else:\n",
        "            return []\n",
        "        cleaned_items = []\n",
        "        for item in item_list:\n",
        "            if is_system_item(item):\n",
        "                continue\n",
        "            name_to_clean = None\n",
        "            if isinstance(item, dict) and 'item_name' in item:\n",
        "                name_to_clean = item['item_name']\n",
        "            elif isinstance(item, str):\n",
        "                name_to_clean = item\n",
        "            if name_to_clean:\n",
        "                cleaned_name = clean_item_name(name_to_clean)\n",
        "                if cleaned_name:\n",
        "                    cleaned_items.append(cleaned_name)\n",
        "        return cleaned_items\n",
        "    except (json.JSONDecodeError, TypeError, KeyError, IndexError):\n",
        "        return []\n",
        "\n",
        "# --- Apply Preprocessing ---\n",
        "print(\"Applying new preprocessing strategy...\")\n",
        "\n",
        "# 1. Merge all data sources into one DataFrame\n",
        "print(\"Merging order, customer, and store data...\")\n",
        "merged_data = pd.merge(order_data, customer_data, on='CUSTOMER_ID', how='left')\n",
        "merged_data = pd.merge(merged_data, store_data, on='STORE_NUMBER', how='left')\n",
        "\n",
        "# Handle missing values for contextual features\n",
        "merged_data['CUSTOMER_TYPE'].fillna('Guest', inplace=True)\n",
        "merged_data['CITY'].fillna('Unknown', inplace=True)\n",
        "merged_data['ORDER_OCCASION_NAME'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# 2. Parse the 'ORDERS' JSON column\n",
        "print(\"Parsing order items from JSON...\")\n",
        "merged_data['parsed_orders'] = merged_data['ORDERS'].apply(parse_order_json)\n",
        "\n",
        "# 3. Explode the DataFrame to have one row per item\n",
        "# This structure is perfect for creating item profiles AND for Market Basket Analysis\n",
        "full_data = merged_data.explode('parsed_orders').rename(columns={'parsed_orders': 'item_name'}).dropna(subset=['item_name'])\n",
        "\n",
        "# This check prevents errors if the dataframe is empty\n",
        "if not full_data.empty:\n",
        "    # 4. Create the specific, trainable item name\n",
        "    # In this new strategy, this is the cleaned version of the full item name\n",
        "    full_data['specific_item_name'] = full_data['item_name'].apply(clean_item_name)\n",
        "\n",
        "    # 5. Create the new, rich 'item_profile' for the Content-Based model\n",
        "    # This profile combines all the context you requested.\n",
        "    print(\"Creating rich item profiles with context...\")\n",
        "    full_data['item_profile'] = full_data['specific_item_name'] + ' | ' + \\\n",
        "                                full_data['CUSTOMER_TYPE'].str.lower() + ' | ' + \\\n",
        "                                full_data['ORDER_OCCASION_NAME'].str.lower() + ' | ' + \\\n",
        "                                full_data['CITY'].str.lower().str.replace(' ', '')\n",
        "\n",
        "    print(\"\\nâœ… Preprocessing complete!\")\n",
        "    print(f\"Processed item rows: {len(full_data)}\")\n",
        "\n",
        "    # Display the key new columns. This is the data ready for the models.\n",
        "    display_cols = [\n",
        "        'specific_item_name',\n",
        "        'CUSTOMER_TYPE',\n",
        "        'ORDER_OCCASION_NAME',\n",
        "        'CITY',\n",
        "        'item_profile'\n",
        "    ]\n",
        "    print(full_data[display_cols].head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nâŒ CRITICAL ERROR: The 'full_data' DataFrame is empty after processing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2z4uNRHL2u8",
        "outputId": "c407ceb8-af50-4f5e-96d2-4f5b263dbe91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying new preprocessing strategy...\n",
            "Merging order, customer, and store data...\n",
            "Parsing order items from JSON...\n",
            "Creating rich item profiles with context...\n",
            "\n",
            "âœ… Preprocessing complete!\n",
            "Processed item rows: 2084311\n",
            "          specific_item_name CUSTOMER_TYPE ORDER_OCCASION_NAME          CITY  \\\n",
            "0  10 pc grilled wings combo    Registered                ToGo     GRAPEVINE   \n",
            "0   8 pc grilled wings combo    Registered                ToGo     GRAPEVINE   \n",
            "0     8 pc spicy wings combo    Registered                ToGo     GRAPEVINE   \n",
            "1          ranch dip regular    Registered                ToGo  HUNTERSVILLE   \n",
            "1        50 pc grilled wings    Registered                ToGo  HUNTERSVILLE   \n",
            "\n",
            "                                        item_profile  \n",
            "0  10 pc grilled wings combo | registered | togo ...  \n",
            "0  8 pc grilled wings combo | registered | togo |...  \n",
            "0  8 pc spicy wings combo | registered | togo | g...  \n",
            "1  ranch dip regular | registered | togo | hunter...  \n",
            "1  50 pc grilled wings | registered | togo | hunt...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry4Ilm76JcuW"
      },
      "source": [
        "## Part 3: Building the Hybrid Recommendation System Class\n",
        "\n",
        "With our data now clean and feature-rich, we can build the core of our solution. We will encapsulate all our recommendation logic into a single, well-organized Python class: WingsRUsRecommendationSystem.\n",
        "\n",
        "This object-oriented approach is crucial for managing the complexity of our hybrid strategy. The class will house the distinct training methods for all five of our modelsâ€”from the straightforward Popularity and Market Basket models to the advanced, deep-learning-based Factorization Machine and NCF.\n",
        "\n",
        "Finally, it will contain the most important method: a master recommend function that intelligently orchestrates the outputs from all five models, weighting their scores to produce a single, optimized list of recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7OlESorYJcuZ"
      },
      "outputs": [],
      "source": [
        "# --- PyTorch Dataset and Model Definitions  ---\n",
        "class FMDataset(Dataset):\n",
        "    def __init__(self, user_item_pairs, labels):\n",
        "        self.user_item_pairs = user_item_pairs\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.user_item_pairs[idx, 0], self.user_item_pairs[idx, 1], self.labels[idx]\n",
        "\n",
        "class NCFDatasetWithNegativeSampling(Dataset):\n",
        "    def __init__(self, users, items, labels):\n",
        "        self.users = users\n",
        "        self.items = items\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.labels[idx]\n",
        "\n",
        "class FactorizationMachine(nn.Module):\n",
        "    def __init__(self, num_users, num_items, k):\n",
        "        super(FactorizationMachine, self).__init__()\n",
        "        self.user_embed = nn.Embedding(num_users, k)\n",
        "        self.item_embed = nn.Embedding(num_items, k)\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
        "    def forward(self, users, items):\n",
        "        user_v = self.user_embed(users)\n",
        "        item_v = self.item_embed(items)\n",
        "        dot_product = (user_v * item_v).sum(1)\n",
        "        user_b = self.user_bias(users).squeeze()\n",
        "        item_b = self.item_bias(items).squeeze()\n",
        "        return dot_product + user_b + item_b + self.global_bias\n",
        "\n",
        "class NeuralCollaborativeFiltering(nn.Module):\n",
        "    def __init__(self, num_users, num_items, mf_dim, layers):\n",
        "        super(NeuralCollaborativeFiltering, self).__init__()\n",
        "        self.mf_user_embed = nn.Embedding(num_users, mf_dim)\n",
        "        self.mf_item_embed = nn.Embedding(num_items, mf_dim)\n",
        "        self.mlp_user_embed = nn.Embedding(num_users, layers[0] // 2)\n",
        "        self.mlp_item_embed = nn.Embedding(num_items, layers[0] // 2)\n",
        "        self.mlp_layers = nn.ModuleList()\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.mlp_layers.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            self.mlp_layers.append(nn.ReLU())\n",
        "        self.final_layer = nn.Linear(mf_dim + layers[-1], 1)\n",
        "    def forward(self, users, items):\n",
        "        mf_user_v = self.mf_user_embed(users)\n",
        "        mf_item_v = self.mf_item_embed(items)\n",
        "        gmf_out = mf_user_v * mf_item_v\n",
        "        mlp_user_v = self.mlp_user_embed(users)\n",
        "        mlp_item_v = self.mlp_item_embed(items)\n",
        "        mlp_in = torch.cat([mlp_user_v, mlp_item_v], dim=-1)\n",
        "        mlp_out = mlp_in\n",
        "        for layer in self.mlp_layers:\n",
        "            mlp_out = layer(mlp_out)\n",
        "        fusion = torch.cat([gmf_out, mlp_out], dim=-1)\n",
        "        return self.final_layer(fusion).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The WingsRUsRecommendationSystem Class: The Conductor ğŸ¼\n",
        "\n",
        "Think of this class as the conductor of an orchestra. You have five different musicians (your five models), each playing a different instrument and having a unique strength. The WingsRUsRecommendationSystem class is responsible for:\n",
        "\n",
        "    1. Training each musician individually (the _train_* methods).\n",
        "    2. Listening to all of them at once when a recommendation is needed.\n",
        "    3. Deciding whose instrument should be loudest (the dynamic weights) to create the most harmonious and effective final recommendation.\n",
        "\n",
        "By encapsulating everything in a single class, you keep your code organized, reusable, and easy to manage.\n",
        "\n",
        "The Dynamic Weighting Strategy: Personalization vs. Generalization\n",
        "\n",
        "The core idea behind using different weights for Registered and Guest users is to play to the strengths of your models based on the data you have for that specific customer.\n",
        "\n",
        "For Registered Users: A Personalized Experience\n",
        "\n",
        "    1. What You Have: For a registered user, you have a rich purchase history. You know what they've bought in the past, which gives you powerful clues about their personal tastes.\n",
        "    2. The Strategy: The weights for registered users heavily favor the Factorization Machine (FM) model (40% weight). The FM model is the only one that learns a unique \"taste profile\" for each individual user. By giving it the highest weight, you are telling the system: \"For this customer, prioritize what we know about their specific, personal history.\" The other models, like Market Basket Analysis (30%), provide strong complementary suggestions, but the personalization from FM takes the lead.\n",
        "\n",
        "For Guest Users: A Smart, General Approach\n",
        "\n",
        "    1. What You Have: For a guest user, you have no purchase history. You only know what is currently in their cart.\n",
        "    2. The Strategy: The weights for guest users completely turn off the personalized models (FM and NCF have 0% weight) because there is no data to personalize from. Instead, the system relies on the powerful, non-personalized models:\n",
        "\n",
        "        Market Basket Analysis (40%): This becomes the most important model. It doesn't know the user, but it knows that \"people who buy X also buy Y.\" It provides excellent complementary recommendations based on the items in the guest's cart.\n",
        "        Content-Based & Popularity (30% each): These models provide strong, logical suggestions (similar items) and safe, popular choices, which are perfect for a user you know nothing about.\n",
        "\n",
        "This dynamic approach is highly effective because it ensures that every recommendation is generated using the most relevant information available for that specific customer, leading to a smarter and more tailored experience for everyone."
      ],
      "metadata": {
        "id": "sB-FgOC1RudM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WingsRUsRecommendationSystem:\n",
        "    def __init__(self):\n",
        "        # --- FIX: Remove static weights and add customer type dictionary ---\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.customer_types = {} # To store a map of CUSTOMER_ID -> CUSTOMER_TYPE\n",
        "\n",
        "        self.popular_items = []\n",
        "        self.association_rules = pd.DataFrame()\n",
        "        self.tfidf_vectorizer = TfidfVectorizer()\n",
        "        self.tfidf_matrix = None\n",
        "        self.item_encoder = LabelEncoder()\n",
        "        self.user_encoder = LabelEncoder()\n",
        "        self.fm_model = None\n",
        "        self.ncf_model = None\n",
        "        self.item_map = {}\n",
        "\n",
        "    def _train_popularity_model(self, data):\n",
        "        print(\"Training Popularity Model...\")\n",
        "        self.popular_items = data['specific_item_name'].value_counts().nlargest(20).index.tolist()\n",
        "\n",
        "    def _train_market_basket_model(self, data):\n",
        "        print(\"Training Market Basket Model...\")\n",
        "        transactions = data.groupby('ORDER_ID')['specific_item_name'].apply(list).tolist()\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit(transactions).transform(transactions)\n",
        "        df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "        frequent_itemsets = apriori(df, min_support=0.001, use_colnames=True)\n",
        "        if not frequent_itemsets.empty:\n",
        "            rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
        "            self.association_rules = rules.sort_values(by='lift', ascending=False)\n",
        "\n",
        "    def _train_content_model(self, data):\n",
        "        print(\"Training Content-Based Model...\")\n",
        "        item_profiles = data[['specific_item_name', 'item_profile']].drop_duplicates()\n",
        "        self.item_map = item_profiles['specific_item_name'].tolist()\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(item_profiles['item_profile'])\n",
        "\n",
        "    def _train_fm_model(self, train_loader, val_loader, num_users, num_items, epochs=8):\n",
        "        print(\"Training Factorization Machine Model...\")\n",
        "        self.fm_model = FactorizationMachine(num_users, num_items, k=20).to(self.device)\n",
        "        optimizer = optim.Adam(self.fm_model.parameters(), lr=0.01)\n",
        "        criterion = nn.MSELoss()\n",
        "        for epoch in range(epochs):\n",
        "            self.fm_model.train()\n",
        "            total_train_loss = 0\n",
        "            for users, items, _ in train_loader:\n",
        "                users, items = users.to(self.device), items.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.fm_model(users, items)\n",
        "                loss = criterion(outputs, torch.ones(outputs.size(0)).to(self.device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item()\n",
        "            self.fm_model.eval()\n",
        "            total_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for users, items, _ in val_loader:\n",
        "                    users, items = users.to(self.device), items.to(self.device)\n",
        "                    outputs = self.fm_model(users, items)\n",
        "                    val_loss = criterion(outputs, torch.ones(outputs.size(0)).to(self.device))\n",
        "                    total_val_loss += val_loss.item()\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "            print(f\"FM Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    def _train_ncf_model(self, train_loader, val_loader, num_users, num_items, epochs=5):\n",
        "        print(\"Training Neural Collaborative Filtering Model...\")\n",
        "        self.ncf_model = NeuralCollaborativeFiltering(num_users, num_items, mf_dim=20, layers=[64, 32, 16]).to(self.device)\n",
        "        optimizer = optim.Adam(self.ncf_model.parameters(), lr=0.001)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        for epoch in range(epochs):\n",
        "            self.ncf_model.train()\n",
        "            total_train_loss = 0\n",
        "            for users, items, labels in train_loader:\n",
        "                users, items, labels = users.to(self.device), items.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.ncf_model(users, items)\n",
        "                loss = criterion(outputs, labels.float())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item()\n",
        "            self.ncf_model.eval()\n",
        "            total_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for users, items, labels in val_loader:\n",
        "                    users, items, labels = users.to(self.device), items.to(self.device), labels.to(self.device)\n",
        "                    outputs = self.ncf_model(users, items)\n",
        "                    val_loss = criterion(outputs, labels.float())\n",
        "                    total_val_loss += val_loss.item()\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "            print(f\"NCF Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    def _get_market_basket_recs(self, cart_items, n=10):\n",
        "        if self.association_rules.empty: return {}\n",
        "        recs = {}\n",
        "        for item in cart_items:\n",
        "            rules = self.association_rules[self.association_rules['antecedents'].apply(lambda x: item in x)]\n",
        "            for _, rule in rules.iterrows():\n",
        "                for consequent in rule['consequents']:\n",
        "                    if consequent not in cart_items:\n",
        "                        recs[consequent] = max(recs.get(consequent, 0), rule['lift'])\n",
        "        return dict(sorted(recs.items(), key=lambda x: x[1], reverse=True)[:n])\n",
        "\n",
        "    def _get_content_recs(self, cart_items, n=10):\n",
        "        if not cart_items or self.tfidf_matrix is None: return {}\n",
        "        try:\n",
        "            item_indices = [self.item_map.index(item) for item in cart_items if item in self.item_map]\n",
        "            if not item_indices: return {}\n",
        "            avg_vector = self.tfidf_matrix[item_indices].mean(axis=0)\n",
        "            avg_vector_array = np.asarray(avg_vector)\n",
        "            sim_scores = cosine_similarity(avg_vector_array, self.tfidf_matrix).flatten()\n",
        "            top_indices = sim_scores.argsort()[-n-len(cart_items):][::-1]\n",
        "            recs = {self.item_map[i]: sim_scores[i] for i in top_indices if self.item_map[i] not in cart_items}\n",
        "            return recs\n",
        "        except ValueError: return {}\n",
        "\n",
        "    def _get_deep_learning_recs(self, model, user_id, n=10):\n",
        "        if model is None or user_id not in self.user_encoder.classes_: return {}\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            user_idx = self.user_encoder.transform([user_id])[0]\n",
        "            all_item_indices = torch.arange(len(self.item_encoder.classes_)).to(self.device)\n",
        "            user_indices = torch.full_like(all_item_indices, user_idx)\n",
        "            scores = model(user_indices, all_item_indices)\n",
        "            top_scores, top_indices = torch.topk(scores, n + 10)\n",
        "            recs = {}\n",
        "            item_names = self.item_encoder.inverse_transform(top_indices.cpu().numpy())\n",
        "            for item, score in zip(item_names, top_scores.cpu().numpy()):\n",
        "                recs[item] = score\n",
        "            return recs\n",
        "\n",
        "    def fit(self, data, ncf_neg_samples=4):\n",
        "        print(\"--- Starting Training of Hybrid Recommendation System ---\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # --- FIX: Create the customer type lookup dictionary ---\n",
        "        print(\"Creating customer type lookup...\")\n",
        "        self.customer_types = data[['CUSTOMER_ID', 'CUSTOMER_TYPE']].drop_duplicates().set_index('CUSTOMER_ID')['CUSTOMER_TYPE'].to_dict()\n",
        "\n",
        "        self._train_popularity_model(data)\n",
        "        self._train_market_basket_model(data)\n",
        "        self._train_content_model(data)\n",
        "\n",
        "        print(\"\\nPreparing data for deep learning models...\")\n",
        "        self.user_encoder.fit(data['CUSTOMER_ID'])\n",
        "        self.item_encoder.fit(data['specific_item_name'])\n",
        "        data['user_idx'] = self.user_encoder.transform(data['CUSTOMER_ID'])\n",
        "        data['item_idx'] = self.item_encoder.transform(data['specific_item_name'])\n",
        "\n",
        "        train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "        num_users = len(self.user_encoder.classes_)\n",
        "        num_items = len(self.item_encoder.classes_)\n",
        "\n",
        "        # FM DataLoaders\n",
        "        train_dataset_fm = FMDataset(train_data[['user_idx', 'item_idx']].values, np.ones(len(train_data)))\n",
        "        val_dataset_fm = FMDataset(val_data[['user_idx', 'item_idx']].values, np.ones(len(val_data)))\n",
        "        train_loader_fm = DataLoader(train_dataset_fm, batch_size=1024, shuffle=True)\n",
        "        val_loader_fm = DataLoader(val_dataset_fm, batch_size=1024)\n",
        "        self._train_fm_model(train_loader_fm, val_loader_fm, num_users, num_items, epochs=8)\n",
        "\n",
        "        # NCF Data Preparation\n",
        "        print(\"\\nPerforming negative sampling for NCF model...\")\n",
        "        user_item_interactions = data.groupby('user_idx')['item_idx'].apply(set)\n",
        "        users = train_data['user_idx'].tolist()\n",
        "        items = train_data['item_idx'].tolist()\n",
        "        labels = [1] * len(users)\n",
        "        for user_id in train_data['user_idx'].unique():\n",
        "            interacted_items = user_item_interactions.get(user_id, set())\n",
        "            for _ in range(ncf_neg_samples):\n",
        "                while True:\n",
        "                    negative_item = np.random.randint(0, num_items)\n",
        "                    if negative_item not in interacted_items:\n",
        "                        users.append(user_id)\n",
        "                        items.append(negative_item)\n",
        "                        labels.append(0)\n",
        "                        break\n",
        "        train_dataset_ncf = NCFDatasetWithNegativeSampling(users, items, labels)\n",
        "        val_dataset_ncf = NCFDatasetWithNegativeSampling(val_data['user_idx'].values, val_data['item_idx'].values, np.ones(len(val_data)))\n",
        "        train_loader_ncf = DataLoader(train_dataset_ncf, batch_size=1024, shuffle=True)\n",
        "        val_loader_ncf = DataLoader(val_dataset_ncf, batch_size=1024)\n",
        "        self._train_ncf_model(train_loader_ncf, val_loader_ncf, num_users, num_items, epochs=5)\n",
        "\n",
        "        print(f\"--- Training Complete in {time.time() - start_time:.2f} seconds ---\")\n",
        "\n",
        "    def recommend(self, customer_id, cart_items, n=3):\n",
        "        # --- DYNAMIC WEIGHTING IMPLEMENTATION ---\n",
        "        # 1. Determine customer type with fallback to 'Guest'\n",
        "        customer_type = self.customer_types.get(customer_id, 'Guest')\n",
        "\n",
        "        # 2. Set weights based on customer type\n",
        "        if customer_type == 'Registered':\n",
        "            weights = {\n",
        "                'popularity': 0.1,\n",
        "                'market_basket': 0.3,\n",
        "                'content': 0.2,\n",
        "                'fm': 0.4,       # Higher personalization for registered users\n",
        "                'ncf': 0.0       # NCF was overfitting, so its weight is 0 for now\n",
        "            }\n",
        "        else:  # Guest or other types\n",
        "            weights = {\n",
        "                'popularity': 0.3,\n",
        "                'market_basket': 0.4,\n",
        "                'content': 0.3,\n",
        "                'fm': 0.0,       # No personalization for guests\n",
        "                'ncf': 0.0\n",
        "            }\n",
        "        # --- END DYNAMIC WEIGHTING ---\n",
        "\n",
        "        clean_cart = [clean_item_name(item) for item in cart_items if pd.notna(item)]\n",
        "\n",
        "        pop_recs = {item: 1.0 for item in self.popular_items}\n",
        "        mb_recs = self._get_market_basket_recs(clean_cart)\n",
        "        content_recs = self._get_content_recs(clean_cart)\n",
        "        fm_recs = self._get_deep_learning_recs(self.fm_model, customer_id)\n",
        "        ncf_recs = self._get_deep_learning_recs(self.ncf_model, customer_id)\n",
        "\n",
        "        combined_scores = Counter()\n",
        "        all_recs = {'popularity': pop_recs, 'market_basket': mb_recs, 'content': content_recs, 'fm': fm_recs, 'ncf': ncf_recs}\n",
        "\n",
        "        for model_name, recs in all_recs.items():\n",
        "            weight = weights[model_name]\n",
        "            if weight == 0: continue\n",
        "\n",
        "            max_score = max(recs.values()) if recs else 1\n",
        "            for item, score in recs.items():\n",
        "                if item not in clean_cart:\n",
        "                    normalized_score = score / max_score\n",
        "                    combined_scores[item] += weight * normalized_score\n",
        "\n",
        "        if not combined_scores:\n",
        "            final_recs = [item for item in self.popular_items if item not in clean_cart]\n",
        "        else:\n",
        "            sorted_recs = combined_scores.most_common()\n",
        "            final_recs = [item for item, score in sorted_recs]\n",
        "\n",
        "        return (final_recs + self.popular_items)[:n]"
      ],
      "metadata": {
        "id": "ZKKEG-WyLRjw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRSNnhAVJcua"
      },
      "source": [
        "## Part 4: Training the System\n",
        "\n",
        "With the class defined, we can now instantiate it and call the `.fit()` method. This single command will orchestrate the training of all five sub-models, preparing the system to generate recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92GbNEnUJcuc",
        "outputId": "6fa31b41-8ae7-4660-a6a7-270ca730f5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data size: 2084311. Sampling down to 800,000 rows to conserve memory.\n",
            "Training will proceed with 800000 data points.\n",
            "Using device: cuda\n",
            "--- Starting Training of Hybrid Recommendation System ---\n",
            "Creating customer type lookup...\n",
            "Training Popularity Model...\n",
            "Training Market Basket Model...\n",
            "Training Content-Based Model...\n",
            "\n",
            "Preparing data for deep learning models...\n",
            "Training Factorization Machine Model...\n",
            "FM Epoch 1/8, Train Loss: 3.7809, Val Loss: 0.8263\n",
            "FM Epoch 2/8, Train Loss: 0.5515, Val Loss: 0.5735\n",
            "FM Epoch 3/8, Train Loss: 0.2695, Val Loss: 0.4504\n",
            "FM Epoch 4/8, Train Loss: 0.1243, Val Loss: 0.3894\n",
            "FM Epoch 5/8, Train Loss: 0.0634, Val Loss: 0.3587\n",
            "FM Epoch 6/8, Train Loss: 0.0390, Val Loss: 0.3406\n",
            "FM Epoch 7/8, Train Loss: 0.0283, Val Loss: 0.3267\n",
            "FM Epoch 8/8, Train Loss: 0.0233, Val Loss: 0.3159\n",
            "\n",
            "Performing negative sampling for NCF model...\n",
            "Training Neural Collaborative Filtering Model...\n",
            "NCF Epoch 1/5, Train Loss: 0.4343, Val Loss: 0.5939\n",
            "NCF Epoch 2/5, Train Loss: 0.4153, Val Loss: 0.6061\n",
            "NCF Epoch 3/5, Train Loss: 0.4040, Val Loss: 0.6185\n",
            "NCF Epoch 4/5, Train Loss: 0.3802, Val Loss: 0.6305\n",
            "NCF Epoch 5/5, Train Loss: 0.3485, Val Loss: 0.6133\n",
            "--- Training Complete in 194.03 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#  Reduce data size to prevent memory overload ---\n",
        "# Create a smaller, random sample of the data for training.\n",
        "\n",
        "if len(full_data) > 800000:\n",
        "    print(f\"Original data size: {len(full_data)}. Sampling down to 800,000 rows to conserve memory.\")\n",
        "    training_data = full_data.sample(n=800000, random_state=42)\n",
        "else:\n",
        "    training_data = full_data\n",
        "\n",
        "print(f\"Training will proceed with {len(training_data)} data points.\")\n",
        "\n",
        "# Instantiate the recommendation system\n",
        "recsys = WingsRUsRecommendationSystem()\n",
        "\n",
        "# Train the system on our sampled, preprocessed data\n",
        "recsys.fit(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53niTcqbJcuc"
      },
      "source": [
        "## Part 5: Generating and Saving the Final Submission File\n",
        "\n",
        "This is the final step. We will iterate through the `test_data_question.csv` file. For each row, we'll take the provided items as the customer's current cart and use our trained system to generate the top 3 recommendations. The results will be stored in a new DataFrame and saved as `recommendation.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "jN4e4CjWJcud",
        "outputId": "a4bb3d5e-e278-4f58-ebe0-81161439ec71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating recommendations for the test data...\n",
            "Generation complete in 13.88 seconds.\n",
            "\n",
            "âœ… Submission file 'Crtl+Z_Mandal_Recommendation Output Sheet.csv' created successfully in the correct format!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   CUSTOMER_ID    ORDER_ID                     item1                item2  \\\n",
              "0    997177535  9351345556         Chicken Sub Combo  Ranch Dip - Regular   \n",
              "1    345593831  3595377080     Regular Buffalo Fries    10 pc Spicy Wings   \n",
              "2    160955031  4071757785       Large Buffalo Fries    10 pc Spicy Wings   \n",
              "3    890671991  3931766769  6 pc Grilled Wings Combo  20 pc Grilled Wings   \n",
              "4     73989021  3739700809     Regular Buffalo Fries  20 pc Grilled Wings   \n",
              "\n",
              "                      item3         RECOMMENDATION 1     RECOMMENDATION 2  \\\n",
              "0   10 pc Spicy Wings Combo        10 pc spicy wings  10 pc grilled wings   \n",
              "1  3 pc Crispy Strips Combo  10 pc spicy wings combo  10 pc grilled wings   \n",
              "2       Ranch Dip - Regular    regular buffalo fries      ranch dip large   \n",
              "3        Fried Corn - Large    regular buffalo fries  large buffalo fries   \n",
              "4         Ranch Dip - Large      large buffalo fries    ranch dip regular   \n",
              "\n",
              "         RECOMMENDATION 3  \n",
              "0  6 pc spicy wings combo  \n",
              "1       ranch dip regular  \n",
              "2     10 pc grilled wings  \n",
              "3         ranch dip large  \n",
              "4       10 pc spicy wings  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7c22f65-bcd1-4052-81a4-3c881b9be0de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CUSTOMER_ID</th>\n",
              "      <th>ORDER_ID</th>\n",
              "      <th>item1</th>\n",
              "      <th>item2</th>\n",
              "      <th>item3</th>\n",
              "      <th>RECOMMENDATION 1</th>\n",
              "      <th>RECOMMENDATION 2</th>\n",
              "      <th>RECOMMENDATION 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>997177535</td>\n",
              "      <td>9351345556</td>\n",
              "      <td>Chicken Sub Combo</td>\n",
              "      <td>Ranch Dip - Regular</td>\n",
              "      <td>10 pc Spicy Wings Combo</td>\n",
              "      <td>10 pc spicy wings</td>\n",
              "      <td>10 pc grilled wings</td>\n",
              "      <td>6 pc spicy wings combo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>345593831</td>\n",
              "      <td>3595377080</td>\n",
              "      <td>Regular Buffalo Fries</td>\n",
              "      <td>10 pc Spicy Wings</td>\n",
              "      <td>3 pc Crispy Strips Combo</td>\n",
              "      <td>10 pc spicy wings combo</td>\n",
              "      <td>10 pc grilled wings</td>\n",
              "      <td>ranch dip regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>160955031</td>\n",
              "      <td>4071757785</td>\n",
              "      <td>Large Buffalo Fries</td>\n",
              "      <td>10 pc Spicy Wings</td>\n",
              "      <td>Ranch Dip - Regular</td>\n",
              "      <td>regular buffalo fries</td>\n",
              "      <td>ranch dip large</td>\n",
              "      <td>10 pc grilled wings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>890671991</td>\n",
              "      <td>3931766769</td>\n",
              "      <td>6 pc Grilled Wings Combo</td>\n",
              "      <td>20 pc Grilled Wings</td>\n",
              "      <td>Fried Corn - Large</td>\n",
              "      <td>regular buffalo fries</td>\n",
              "      <td>large buffalo fries</td>\n",
              "      <td>ranch dip large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>73989021</td>\n",
              "      <td>3739700809</td>\n",
              "      <td>Regular Buffalo Fries</td>\n",
              "      <td>20 pc Grilled Wings</td>\n",
              "      <td>Ranch Dip - Large</td>\n",
              "      <td>large buffalo fries</td>\n",
              "      <td>ranch dip regular</td>\n",
              "      <td>10 pc spicy wings</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7c22f65-bcd1-4052-81a4-3c881b9be0de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7c22f65-bcd1-4052-81a4-3c881b9be0de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7c22f65-bcd1-4052-81a4-3c881b9be0de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3023d594-508d-4593-928b-067708dcd62e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3023d594-508d-4593-928b-067708dcd62e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3023d594-508d-4593-928b-067708dcd62e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(final_submission_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"CUSTOMER_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 424231170,\n        \"min\": 73989021,\n        \"max\": 997177535,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          345593831,\n          73989021,\n          160955031\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ORDER_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2473815733,\n        \"min\": 3595377080,\n        \"max\": 9351345556,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3595377080,\n          3739700809,\n          4071757785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"item1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Regular Buffalo Fries\",\n          \"6 pc Grilled Wings Combo\",\n          \"Chicken Sub Combo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"item2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Ranch Dip - Regular\",\n          \"10 pc Spicy Wings\",\n          \"20 pc Grilled Wings\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"item3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"3 pc Crispy Strips Combo\",\n          \"Ranch Dip - Large\",\n          \"Ranch Dip - Regular\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RECOMMENDATION 1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"10 pc spicy wings combo\",\n          \"large buffalo fries\",\n          \"10 pc spicy wings\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RECOMMENDATION 2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"ranch dip large\",\n          \"ranch dip regular\",\n          \"10 pc grilled wings\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RECOMMENDATION 3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ranch dip regular\",\n          \"10 pc spicy wings\",\n          \"10 pc grilled wings\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "print(\"Generating recommendations for the test data...\")\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for _, row in test_data.iterrows():\n",
        "    customer_id = row['CUSTOMER_ID']\n",
        "    # Combine all item columns to form the cart\n",
        "    cart = [item for item in [row.get('item1'), row.get('item2'), row.get('item3')] if pd.notna(item)]\n",
        "\n",
        "    # Get top 3 recommendations\n",
        "    recommendations = recsys.recommend(customer_id, cart, n=3)\n",
        "\n",
        "    # Append to results list\n",
        "    result_row = row.to_dict()\n",
        "    result_row['RECOMMENDATION 1'] = recommendations[0] if len(recommendations) > 0 else 'N/A'\n",
        "    result_row['RECOMMENDATION 2'] = recommendations[1] if len(recommendations) > 1 else 'N/A'\n",
        "    result_row['RECOMMENDATION 3'] = recommendations[2] if len(recommendations) > 2 else 'N/A'\n",
        "    results.append(result_row)\n",
        "\n",
        "print(f\"Generation complete in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "full_results_df = pd.DataFrame(results)\n",
        "\n",
        "# --- FIX: Define the exact columns needed for the final submission format ---\n",
        "submission_columns = [\n",
        "    'CUSTOMER_ID', 'ORDER_ID',\n",
        "    'item1', 'item2', 'item3',\n",
        "    'RECOMMENDATION 1', 'RECOMMENDATION 2', 'RECOMMENDATION 3'\n",
        "]\n",
        "\n",
        "# --- Select only those columns to create the final DataFrame ---\n",
        "# This ensures the output matches the required format precisely.\n",
        "\n",
        "final_submission_df = full_results_df[submission_columns]\n",
        "\n",
        "\n",
        "# Save the final, formatted DataFrame to CSV\n",
        "output_filename = 'Crtl+Z_Mandal_Recommendation Output Sheet.csv'\n",
        "final_submission_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Submission file '{output_filename}' created successfully in the correct format!\")\n",
        "display(final_submission_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "print(\"--- Starting Self-Evaluation with Hold-out Set ---\")\n",
        "\n",
        "if 'full_data' in locals() and not full_data.empty:\n",
        "    baskets = full_data.groupby('ORDER_ID')['specific_item_name'].apply(list).tolist()\n",
        "    baskets = [b for b in baskets if len(b) > 1]\n",
        "    print(f\"Found {len(baskets)} orders with more than one item.\")\n",
        "\n",
        "    train_baskets, validation_baskets = train_test_split(baskets, test_size=0.2, random_state=42)\n",
        "    print(f\"Split into {len(train_baskets)} training baskets and {len(validation_baskets)} validation baskets.\")\n",
        "\n",
        "    # --- FIX 1: Sample the TRAINING baskets to prevent memory overload ---\n",
        "    if len(train_baskets) > 100000:\n",
        "        print(f\"\\nTraining set is large. Sampling 100,000 baskets for a faster, memory-safe model build.\")\n",
        "        train_baskets_sample = random.sample(train_baskets, 100000)\n",
        "    else:\n",
        "        train_baskets_sample = train_baskets\n",
        "\n",
        "    # --- FIX 2: Sample the VALIDATION baskets for a quick evaluation ---\n",
        "    if len(validation_baskets) > 20000:\n",
        "        print(f\"Validation set is large. Sampling 20,000 baskets for evaluation.\")\n",
        "        validation_baskets_sample = random.sample(validation_baskets, 20000)\n",
        "    else:\n",
        "        validation_baskets_sample = validation_baskets\n",
        "\n",
        "    print(\"\\nTraining a new model instance on the training sample...\")\n",
        "    validation_recsys = WingsRUsRecommendationSystem()\n",
        "\n",
        "    # Use the sampled training baskets to create the DataFrame\n",
        "    train_df_for_validation = pd.DataFrame({\n",
        "        'ORDER_ID': [i for i, basket in enumerate(train_baskets_sample) for _ in basket],\n",
        "        'specific_item_name': [item for basket in train_baskets_sample for item in basket]\n",
        "    })\n",
        "    train_df_for_validation['CUSTOMER_ID'] = 0\n",
        "    train_df_for_validation['item_profile'] = train_df_for_validation['specific_item_name']\n",
        "\n",
        "    validation_recsys._train_popularity_model(train_df_for_validation)\n",
        "    validation_recsys._train_market_basket_model(train_df_for_validation)\n",
        "    validation_recsys._train_content_model(train_df_for_validation)\n",
        "\n",
        "    print(\"\\n--- Evaluating model on the validation sample ---\")\n",
        "    hits = 0\n",
        "    # Use the sampled validation baskets for the loop\n",
        "    for basket in validation_baskets_sample:\n",
        "        cart = basket[:-1]\n",
        "        ground_truth = basket[-1]\n",
        "        recommendations = validation_recsys.recommend(customer_id=0, cart_items=cart, n=3)\n",
        "        if ground_truth in recommendations:\n",
        "            hits += 1\n",
        "\n",
        "    recall_at_3 = hits / len(validation_baskets_sample) if len(validation_baskets_sample) > 0 else 0\n",
        "\n",
        "    print(\"\\n--- Evaluation Complete ---\")\n",
        "    print(f\"Total Validation Orders Evaluated: {len(validation_baskets_sample)}\")\n",
        "    print(f\"Correct Recommendations (Hits): {hits}\")\n",
        "    print(f\"Estimated Recall@3 Score: {recall_at_3:.2%}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Error: `full_data` DataFrame not found or is empty. Please run the preprocessing steps first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMaWKosOL8U9",
        "outputId": "e03f39d5-f1fe-4814-af62-ae0536477c80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Self-Evaluation with Hold-out Set ---\n",
            "Found 600341 orders with more than one item.\n",
            "Split into 480272 training baskets and 120069 validation baskets.\n",
            "\n",
            "Training set is large. Sampling 100,000 baskets for a faster, memory-safe model build.\n",
            "Validation set is large. Sampling 20,000 baskets for evaluation.\n",
            "\n",
            "Training a new model instance on the training sample...\n",
            "Using device: cuda\n",
            "Training Popularity Model...\n",
            "Training Market Basket Model...\n",
            "Training Content-Based Model...\n",
            "\n",
            "--- Evaluating model on the validation sample ---\n",
            "\n",
            "--- Evaluation Complete ---\n",
            "Total Validation Orders Evaluated: 20000\n",
            "Correct Recommendations (Hits): 4726\n",
            "Estimated Recall@3 Score: 23.63%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}