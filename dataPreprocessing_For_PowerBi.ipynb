{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q22jTSwnymUd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast  # For safely evaluating string-formatted lists\n",
        "import json # For handling nested JSON\n",
        "import sys  # For error checking\n",
        "from google.colab import files # Import Colab's file handling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7TpvrYvRRLG",
        "outputId": "2738b30c-8ef8-41eb-aa46-573b654b4bd6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "order_path = '/content/drive/MyDrive/WWT Dataset/order_data.csv'\n",
        "customer_path = '/content/drive/MyDrive/WWT Dataset/customer_data.csv'\n",
        "store_path = '/content/drive/MyDrive/WWT Dataset/store_data.csv'\n",
        "\n",
        "try:\n",
        "    print(\"Loading datasets from Google Drive...\")\n",
        "    order_data = pd.read_csv(order_path)\n",
        "    customer_data = pd.read_csv(customer_path)\n",
        "    store_data = pd.read_csv(store_path)\n",
        "\n",
        "    print(\"All files loaded successfully.\")\n",
        "    print(f\"Loaded {len(order_data)} orders.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"--- ERROR ---\")\n",
        "    print(f\"File not found. Did you paste the correct path?\")\n",
        "    print(f\"Path I tried to use: {e.filename}\")\n",
        "    sys.exit()"
      ],
      "metadata": {
        "id": "OYKcRtXZu2il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe046002-c9f2-43d2-fba2-6ee09345261e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets from Google Drive...\n",
            "All files loaded successfully.\n",
            "Loaded 1048575 orders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- This is the main processing script  ---\n",
        "\n",
        "import pandas as pd\n",
        "import ast\n",
        "import json\n",
        "import sys\n",
        "import numpy as np # Import numpy for keyword search\n",
        "\n",
        "print(\"--- Starting Data Preparation   ---\")\n",
        "\n",
        "# Check if 'order_data' exists from Cell 2\n",
        "if 'order_data' not in locals():\n",
        "    print(\"ERROR: 'order_data' not found. Please re-run Cell 2 to load the data.\")\n",
        "else:\n",
        "    try:\n",
        "        # --- 2. CLEAN & TRANSFORM (The Fast Way) ---\n",
        "        print(f\"Original 'order_data' rows: {len(order_data)}\")\n",
        "        print(\"Transforming 'ORDERS' column...\")\n",
        "\n",
        "        print(\"Step 2a (Filling NaNs and ast.literal_eval)...\")\n",
        "        order_data['ORDERS'] = order_data['ORDERS'].fillna('{\"orders\": []}')\n",
        "        order_data['ORDERS_dict'] = order_data['ORDERS'].apply(ast.literal_eval)\n",
        "        print(\"Step 2a: COMPLETE.\")\n",
        "\n",
        "        print(\"Step 2b (Normalizing with record_path)...\")\n",
        "        meta_cols = [\n",
        "            'CUSTOMER_ID', 'STORE_NUMBER', 'ORDER_CREATED_DATE',\n",
        "            'ORDER_ID', 'ORDER_CHANNEL_NAME',\n",
        "            'ORDER_SUBCHANNEL_NAME', 'ORDER_OCCASION_NAME'\n",
        "        ]\n",
        "\n",
        "        final_data = pd.json_normalize(\n",
        "            order_data.to_dict('records'),\n",
        "            record_path=['ORDERS_dict', 'orders', 'item_details'],\n",
        "            meta=meta_cols,\n",
        "            errors='ignore'\n",
        "        )\n",
        "        print(f\"Step 2b: COMPLETE. New expanded row count is: {len(final_data)}\")\n",
        "\n",
        "        # --- !! ADVANCED FILTERING (Keywords and Price) !! ---\n",
        "        print(f\"Filtering out non-product 'noise' rows...\")\n",
        "        rows_before = len(final_data)\n",
        "\n",
        "        # 1. Filter by keywords (Your list)\n",
        "        non_product_keywords = ['memo', 'blankline', 'asap', 'paid', 'not paid', 'tax', 'fee']\n",
        "        keyword_mask = final_data['item_name'].str.lower().str.contains(\n",
        "            '|'.join(non_product_keywords),\n",
        "            na=False\n",
        "        )\n",
        "        final_data = final_data[~keyword_mask]\n",
        "        rows_after_keywords = len(final_data)\n",
        "        print(f\"Removed {rows_before - rows_after_keywords} keyword 'noise' rows.\")\n",
        "\n",
        "\n",
        "        # 2. Filter by zero price (Your new logic)\n",
        "        print(\"Filtering out zero-price items...\")\n",
        "        final_data = final_data[final_data['item_price'] > 0]\n",
        "        rows_after_price = len(final_data)\n",
        "        print(f\"Removed {rows_after_keywords - rows_after_price} zero-price rows.\")\n",
        "\n",
        "        print(f\"Filtering: COMPLETE. Total rows removed: {rows_before - rows_after_price}\")\n",
        "        # --- End of filtering step ---\n",
        "\n",
        "        # --- 3. MERGE with Customer and Store Data ---\n",
        "        print(\"Step 3 (Merging customer/store data)...\")\n",
        "        final_data = pd.merge(final_data, customer_data, on='CUSTOMER_ID', how='left')\n",
        "        final_data = pd.merge(final_data, store_data, on='STORE_NUMBER', how='left')\n",
        "        print(\"Step 3: COMPLETE. All data merged.\")\n",
        "\n",
        "        # --- 4. FEATURE ENGINEERING & CLEANUP ---\n",
        "        print(\"Step 4 (Adding Date features)...\")\n",
        "        final_data['ORDER_CREATED_DATE'] = pd.to_datetime(final_data['ORDER_CREATED_DATE'])\n",
        "        final_data['Order_Day_of_Week'] = final_data['ORDER_CREATED_DATE'].dt.day_name()\n",
        "        final_data['Order_Month'] = final_data['ORDER_CREATED_DATE'].dt.month_name()\n",
        "        final_data['Order_Hour'] = final_data['ORDER_CREATED_DATE'].dt.hour\n",
        "        final_data['Order_Date'] = final_data['ORDER_CREATED_DATE'].dt.date\n",
        "        print(\"Step 4: COMPLETE.\")\n",
        "\n",
        "        # --- 5. EXPORT TO CSV ---\n",
        "        output_filename = 'wings_r_us_powerbi_master_FINAL_CLEAN.csv'\n",
        "        final_data.to_csv(output_filename, index=False)\n",
        "\n",
        "        print(\"\\n--- ðŸ¥³ SUCCESS! ---\")\n",
        "        print(f\"Your file is ready: {output_filename}\")\n",
        "        print(f\"Total Rows in new file: {len(final_data)}\")\n",
        "        print(\"This file is fully cleaned and ready for Power BI.\")\n",
        "        print(\"You can now run Cell 4 to download this new file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n--- ðŸ˜­ ERROR ---\")\n",
        "        print(\"The script failed. Here is the error message:\")\n",
        "        print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GpjbHZxu3ak",
        "outputId": "ac44cc79-fbe2-48c7-b681-a16ee9aa0286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Data Preparation   ---\n",
            "Original 'order_data' rows: 1048575\n",
            "Transforming 'ORDERS' column...\n",
            "Step 2a (Filling NaNs and ast.literal_eval)...\n",
            "Step 2a: COMPLETE.\n",
            "Step 2b (Normalizing with record_path)...\n",
            "Step 2b: COMPLETE. New expanded row count is: 4180341\n",
            "Filtering out non-product 'noise' rows...\n",
            "Removed 2096030 keyword 'noise' rows.\n",
            "Filtering out zero-price items...\n",
            "Removed 6078 zero-price rows.\n",
            "Filtering: COMPLETE. Total rows removed: 2102108\n",
            "Step 3 (Merging customer/store data)...\n",
            "Step 3: COMPLETE. All data merged.\n",
            "Step 4 (Adding Date features)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-67415164.py:72: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  final_data['ORDER_CREATED_DATE'] = pd.to_datetime(final_data['ORDER_CREATED_DATE'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4: COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41IufaO0xcmZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}